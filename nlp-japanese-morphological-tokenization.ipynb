{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/saurabh8112/nlp-japanese-morphological-tokenization?scriptVersionId=172766400\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"8f3e4f4c","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.0064,"end_time":"2024-04-18T23:22:50.400464","exception":false,"start_time":"2024-04-18T23:22:50.394064","status":"completed"},"tags":[]},"source":["# Morphological splitting of japanese sentences into words"]},{"cell_type":"markdown","id":"5d64fb2d","metadata":{"papermill":{"duration":0.006024,"end_time":"2024-04-18T23:22:50.413171","exception":false,"start_time":"2024-04-18T23:22:50.407147","status":"completed"},"tags":[]},"source":["In contrast to English, Japanese (and Chinese) lacks spaces between words in sentences. This characteristic poses a significant challenge for natural language processing tasks, particularly for tokenization. \n","\n","Tokenization is the process of breaking down text into individual words or subwords, which is a crucial step for various NLP models like BERT or GPT that rely on sub-word tokenizers such as **WordPiece or Byte Pair Encoding (BPE)**.\n","\n","Consider the Japanese sentence: **彼女は日本語を勉強しています (She is studying Japanese)**. Unlike in English, where we can easily split the sentence into words using spaces, Japanese sentences require more sophisticated methods for segmentation. This sentence can be split, for example, in these two ways\n","\n","* 彼女 (she) / は (is) / 日本語 (Japanese) / を (object marker) / 勉強 (study) / しています (is doing)\n","* 彼女 (she) / は (is) / 日本語を (Japanese) / 勉強しています (is studying)\n","\n","How do we split this sentence into words? This is where morphological analysis of text comes into picture. In this notebook I will try to give high level overview and usage of some popular moropholical analyzers available.\n","\n","Note: *This initial step of segmenting Japanese text into its constituent units is essential before proceeding with training models like BERT(emebedding generation) or GPT(text generation). Without a reliable tokenization strategy, these models cannot effectively process Japanese language data.*"]},{"cell_type":"markdown","id":"12d9759d","metadata":{"papermill":{"duration":0.005854,"end_time":"2024-04-18T23:22:50.424994","exception":false,"start_time":"2024-04-18T23:22:50.41914","status":"completed"},"tags":[]},"source":["# Tokenization with MeCab and fugashi\n","\n","Mecab is a popular tool for morphological analysis of text. Fugashi is a CPython wrapper over MeCab"]},{"cell_type":"markdown","id":"df443882","metadata":{"papermill":{"duration":0.005588,"end_time":"2024-04-18T23:22:50.437021","exception":false,"start_time":"2024-04-18T23:22:50.431433","status":"completed"},"tags":[]},"source":["Let's install MeCab, Fugashi and iPadic (dictionary distributed with MeCab)"]},{"cell_type":"code","execution_count":1,"id":"588d6791","metadata":{"execution":{"iopub.execute_input":"2024-04-18T23:22:50.450544Z","iopub.status.busy":"2024-04-18T23:22:50.450143Z","iopub.status.idle":"2024-04-18T23:23:40.997183Z","shell.execute_reply":"2024-04-18T23:23:40.995949Z"},"papermill":{"duration":50.557135,"end_time":"2024-04-18T23:23:41.000099","exception":false,"start_time":"2024-04-18T23:22:50.442964","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting fugashi\r\n","  Downloading fugashi-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\r\n","Downloading fugashi-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (600 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m600.9/600.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: fugashi\r\n","Successfully installed fugashi-1.3.2\r\n","Collecting unidic-lite\r\n","  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n","\u001b[?25hBuilding wheels for collected packages: unidic-lite\r\n","  Building wheel for unidic-lite (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25h  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658817 sha256=dc660439fe3d2880afcbdd19a906021ec06d3261c6604cc0ac4edd817a7f0cce\r\n","  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\r\n","Successfully built unidic-lite\r\n","Installing collected packages: unidic-lite\r\n","Successfully installed unidic-lite-1.0.8\r\n"]}],"source":["!pip install fugashi\n","!pip install unidic-lite"]},{"cell_type":"code","execution_count":2,"id":"0a2638db","metadata":{"execution":{"iopub.execute_input":"2024-04-18T23:23:41.022058Z","iopub.status.busy":"2024-04-18T23:23:41.021592Z","iopub.status.idle":"2024-04-18T23:24:19.548646Z","shell.execute_reply":"2024-04-18T23:24:19.546977Z"},"papermill":{"duration":38.541849,"end_time":"2024-04-18T23:24:19.551505","exception":false,"start_time":"2024-04-18T23:23:41.009656","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ipadic\r\n","  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n","\u001b[?25hBuilding wheels for collected packages: ipadic\r\n","  Building wheel for ipadic (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25h  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=edabc509d12c48109b44af35d8a24f0b7f511a199adcdaadd560ade27ea8b430\r\n","  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\r\n","Successfully built ipadic\r\n","Installing collected packages: ipadic\r\n","Successfully installed ipadic-1.0.0\r\n","Collecting mecab-python3\r\n","  Downloading mecab_python3-1.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\r\n","Downloading mecab_python3-1.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: mecab-python3\r\n","Successfully installed mecab-python3-1.0.9\r\n"]}],"source":["!pip install ipadic\n","!pip install mecab-python3"]},{"cell_type":"markdown","id":"8d12783c","metadata":{"papermill":{"duration":0.011869,"end_time":"2024-04-18T23:24:19.575265","exception":false,"start_time":"2024-04-18T23:24:19.563396","status":"completed"},"tags":[]},"source":["# Morphological tokenization!\n","Let's tokenize the sentence with MeCab"]},{"cell_type":"code","execution_count":3,"id":"cf0683c2","metadata":{"execution":{"iopub.execute_input":"2024-04-18T23:24:19.603995Z","iopub.status.busy":"2024-04-18T23:24:19.603524Z","iopub.status.idle":"2024-04-18T23:24:19.623684Z","shell.execute_reply":"2024-04-18T23:24:19.622444Z"},"papermill":{"duration":0.038537,"end_time":"2024-04-18T23:24:19.626541","exception":false,"start_time":"2024-04-18T23:24:19.588004","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["['彼女',\n"," 'カノジョ',\n"," 'カノジョ',\n"," '彼女',\n"," '代名詞',\n"," '1',\n"," 'は',\n"," 'ワ',\n"," 'ハ',\n"," 'は',\n"," '助詞-係助詞',\n"," '日本',\n"," 'ニッポン',\n"," 'ニッポン',\n"," '日本',\n"," '名詞-固有名詞-地名-国',\n"," '3',\n"," '語',\n"," 'ゴ',\n"," 'ゴ',\n"," '語',\n"," '名詞-普通名詞-一般',\n"," '1',\n"," 'を',\n"," 'オ',\n"," 'ヲ',\n"," 'を',\n"," '助詞-格助詞',\n"," '勉強',\n"," 'ベンキョー',\n"," 'ベンキョウ',\n"," '勉強',\n"," '名詞-普通名詞-サ変可能',\n"," '0',\n"," 'し',\n"," 'シ',\n"," 'スル',\n"," '為る',\n"," '動詞-非自立可能',\n"," 'サ行変格',\n"," '連用形-一般',\n"," '0',\n"," 'て',\n"," 'テ',\n"," 'テ',\n"," 'て',\n"," '助詞-接続助詞',\n"," 'い',\n"," 'イ',\n"," 'イル',\n"," '居る',\n"," '動詞-非自立可能',\n"," '上一段-ア行',\n"," '連用形-一般',\n"," '0',\n"," 'ます',\n"," 'マス',\n"," 'マス',\n"," 'ます',\n"," '助動詞',\n"," '助動詞-マス',\n"," '終止形-一般',\n"," 'EOS']"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import MeCab\n","\n","text = \"彼女は日本語を勉強しています\"\n","wakati = MeCab.Tagger()\n","wakati.parse(text).split()"]},{"cell_type":"markdown","id":"cc36ac38","metadata":{"papermill":{"duration":0.012008,"end_time":"2024-04-18T23:24:19.65112","exception":false,"start_time":"2024-04-18T23:24:19.639112","status":"completed"},"tags":[]},"source":["## Wait that's too much information\n","\n","MeCab provides detailed linguistic analysis for each token in the input sentence. It contains surface form, reading, base form, part of speech and other featues. We don't need all of that if we want to use japanese language for text generation or embedding generation.\n","\n"]},{"cell_type":"markdown","id":"90d289c9","metadata":{"papermill":{"duration":0.011664,"end_time":"2024-04-18T23:24:19.67473","exception":false,"start_time":"2024-04-18T23:24:19.663066","status":"completed"},"tags":[]},"source":["## Let's focus only on the essentials\n","This is too much it has a lot of information we don't want. If we only need text then we can add a flag `-Owakati`\n","\n","Adding -Owakati as an argument to the MeCab tagger specifies the output format as the tokenized text only, without additional linguistic information such as readings, part-of-speech tags, or other features. "]},{"cell_type":"code","execution_count":4,"id":"41ba22a5","metadata":{"execution":{"iopub.execute_input":"2024-04-18T23:24:19.700709Z","iopub.status.busy":"2024-04-18T23:24:19.70005Z","iopub.status.idle":"2024-04-18T23:24:19.709882Z","shell.execute_reply":"2024-04-18T23:24:19.708812Z"},"papermill":{"duration":0.025524,"end_time":"2024-04-18T23:24:19.712252","exception":false,"start_time":"2024-04-18T23:24:19.686728","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["['彼女', 'は', '日本', '語', 'を', '勉強', 'し', 'て', 'い', 'ます']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["text = \"彼女は日本語を勉強しています\"\n","wakati = MeCab.Tagger('-Owakati')\n","wakati.parse(text).split()"]},{"cell_type":"markdown","id":"52732676","metadata":{"papermill":{"duration":0.012753,"end_time":"2024-04-18T23:24:19.737135","exception":false,"start_time":"2024-04-18T23:24:19.724382","status":"completed"},"tags":[]},"source":["## With fugashi!"]},{"cell_type":"code","execution_count":5,"id":"a27ca5c3","metadata":{"execution":{"iopub.execute_input":"2024-04-18T23:24:19.764054Z","iopub.status.busy":"2024-04-18T23:24:19.76362Z","iopub.status.idle":"2024-04-18T23:24:19.775171Z","shell.execute_reply":"2024-04-18T23:24:19.773937Z"},"papermill":{"duration":0.027824,"end_time":"2024-04-18T23:24:19.777634","exception":false,"start_time":"2024-04-18T23:24:19.74981","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['彼女', 'は', '日本', '語', 'を', '勉強', 'し', 'て', 'い', 'ます']\n"]}],"source":["\n","import fugashi\n","\n","text = \"彼女は日本語を勉強しています\"\n","tagger = fugashi.Tagger()\n","tokens = [word.surface for word in tagger(text)]\n","print(tokens)\n"]},{"cell_type":"markdown","id":"cd588f63","metadata":{"papermill":{"duration":0.012215,"end_time":"2024-04-18T23:24:19.802135","exception":false,"start_time":"2024-04-18T23:24:19.78992","status":"completed"},"tags":[]},"source":["# Bonus: Tokenization with Fugashi\n","\n","Sudachi is a morphological analyzer based on the double-array trie structure, allowing for efficient dictionary lookup and morphological analysis of Japanese text. \n","\n","It supports multiple dictionaries, including a system dictionary and user-defined dictionaries, and offers features like unknown word handling and customizable tokenization rules.\n","\n"]},{"cell_type":"markdown","id":"07581996","metadata":{"papermill":{"duration":0.01352,"end_time":"2024-04-18T23:24:19.828185","exception":false,"start_time":"2024-04-18T23:24:19.814665","status":"completed"},"tags":[]},"source":["## install fugashi and fugashi-core"]},{"cell_type":"code","execution_count":6,"id":"bdba7d82","metadata":{"execution":{"iopub.execute_input":"2024-04-18T23:24:19.854907Z","iopub.status.busy":"2024-04-18T23:24:19.854239Z","iopub.status.idle":"2024-04-18T23:24:53.575678Z","shell.execute_reply":"2024-04-18T23:24:53.574217Z"},"papermill":{"duration":33.737945,"end_time":"2024-04-18T23:24:53.578589","exception":false,"start_time":"2024-04-18T23:24:19.840644","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sudachipy\r\n","  Downloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\n","Downloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: sudachipy\r\n","Successfully installed sudachipy-0.6.8\r\n","Collecting sudachidict_core\r\n","  Downloading SudachiDict_core-20240409-py3-none-any.whl.metadata (2.5 kB)\r\n","Requirement already satisfied: SudachiPy<0.7,>=0.5 in /opt/conda/lib/python3.10/site-packages (from sudachidict_core) (0.6.8)\r\n","Downloading SudachiDict_core-20240409-py3-none-any.whl (72.0 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: sudachidict_core\r\n","Successfully installed sudachidict_core-20240409\r\n"]}],"source":["!pip install sudachipy\n","!pip install sudachidict_core"]},{"cell_type":"markdown","id":"37827b60","metadata":{"papermill":{"duration":0.014951,"end_time":"2024-04-18T23:24:53.609185","exception":false,"start_time":"2024-04-18T23:24:53.594234","status":"completed"},"tags":[]},"source":["## Split sentence into words"]},{"cell_type":"code","execution_count":7,"id":"ede119fd","metadata":{"execution":{"iopub.execute_input":"2024-04-18T23:24:53.641886Z","iopub.status.busy":"2024-04-18T23:24:53.641434Z","iopub.status.idle":"2024-04-18T23:24:53.728426Z","shell.execute_reply":"2024-04-18T23:24:53.727029Z"},"papermill":{"duration":0.10666,"end_time":"2024-04-18T23:24:53.731156","exception":false,"start_time":"2024-04-18T23:24:53.624496","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['彼女', 'は', '日本語', 'を', '勉強', 'し', 'て', 'い', 'ます']\n"]}],"source":["from sudachipy import tokenizer, dictionary\n","\n","text = \"彼女は日本語を勉強しています\"\n","tokenizer_obj = dictionary.Dictionary().create()\n","tokens = [m.surface() for m in tokenizer_obj.tokenize(text)]\n","print(tokens)\n"]},{"cell_type":"markdown","id":"c83f35ac","metadata":{"papermill":{"duration":0.015957,"end_time":"2024-04-18T23:24:53.763916","exception":false,"start_time":"2024-04-18T23:24:53.747959","status":"completed"},"tags":[]},"source":["## Subtle difference between sudachi and MeCab\n","Its imortant to note that different morophological tokenizers sometimes have slightly different splits for the same sentence. \n","\n","For instance, the sentence \"彼女は日本語を勉強しています\" was split into:\n","* ['彼女', 'は', '日本', '語', 'を', '勉強', 'し', 'て', 'い', 'ます'] by MeCab\n","* ['彼女', 'は', '日本語', 'を', '勉強', 'し', 'て', 'い', 'ます'] by Sudachi\n","\n","The literal 日本語 can be split into:\n","* '日本' (Japan) and '語' (language)\n","* '日本語' (Japanese)\n","\n","\n","And both make equal sense!"]},{"cell_type":"code","execution_count":null,"id":"5e4cf932","metadata":{"papermill":{"duration":0.015215,"end_time":"2024-04-18T23:24:53.794451","exception":false,"start_time":"2024-04-18T23:24:53.779236","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":126.972287,"end_time":"2024-04-18T23:24:54.230821","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-18T23:22:47.258534","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}